{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7250713,"sourceType":"datasetVersion","datasetId":4200932}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow==2.14","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.layers as tfkl\nimport tensorflow.keras.models as tfk\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.tsa.stattools import pacf, acf\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nseed = 42\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.load('/kaggle/input/time-series-training-dataset/training_data.npy')\nvalid_periods = np.load('/kaggle/input/time-series-training-dataset/valid_periods.npy')\ncategories = np.load('/kaggle/input/time-series-training-dataset/categories.npy')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_padding_mask(series, valid_periods):\n    mask = np.zeros_like(series, dtype=bool)\n    for i, (start, end) in enumerate(valid_periods):\n        mask[i, start:end] = True\n    return mask\n\ndef encode_categories(categories):\n    unique_categories = np.unique(categories)\n    category_to_int = {category: i for i, category in enumerate(unique_categories)}\n    return np.array([category_to_int[category] for category in categories])\n\ndef remove_padding(series, valid_period):\n    start, end = valid_period\n    return series[start:end]\n\npadding_mask = create_padding_mask(X, valid_periods)\ncategories=encode_categories(categories)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Thresholds for identifying low and high scales\nlow_threshold = 0.05\nhigh_threshold = 0.9\n\n# Initialize lists to store low and high scale information\nlow_centers = []\nhigh_centers = []\n\nlow_scales = []\nhigh_scales = []\n\n# Row-wise Robust Scaling\nXpredscaled=np.zeros_like(X)\nscalespred = np.zeros(X.shape[0], dtype=float)\ncenterspred = np.zeros(X.shape[0], dtype=float)\n\nfor i in range(X.shape[0]):\n    row_data = X[i, :].reshape(-1, 1)\n    nonzero_mask = X[i, :] != 0\n\n    # Extract non-zero values and reshape to a column vector\n    row_data_nonzero = X[i, nonzero_mask].reshape(-1, 1)\n\n    # Fit the scaler to the non-zero values\n    scaler = RobustScaler().fit(row_data_nonzero)\n\n    # Apply scaling and flatten the result to the corresponding indices\n    Xpredscaled[i, nonzero_mask] = scaler.transform(row_data_nonzero).flatten()\n\n    # Store scale and center for later use\n    scalespred[i] = scaler.scale_[0]\n    centerspred[i] = scaler.center_[0]\n\n    # Categorize scales and store corresponding information\n    if scalespred[i] < low_threshold:\n        low_scales.append(i)\n        \n    if scalespred[i] > high_threshold:\n        high_scales.append(i)\n\n    if centerspred[i] > high_threshold:\n        high_centers.append(i)\n\n    if centerspred[i] < low_threshold:\n        low_centers.append(i)\n        \nlow_scales = np.array(low_scales)\nhigh_scales = np.array(high_scales)\n\nlow_centers = np.array(low_centers)\nhigh_centers = np.array(high_centers)\n\nall_scales = np.union1d(low_scales, high_scales)\nall_centers = np.union1d(low_centers, high_centers)\nall_indexes = np.union1d(all_scales, all_centers)\nlen(all_indexes)\n\nX = np.delete(X, all_indexes, axis=0)\ncategories = np.delete(categories, all_indexes, axis=0)\nvalid_periods = np.delete(valid_periods, all_indexes, axis=0)\n\ntrimmed_data = [remove_padding(X[i], valid_periods[i]) for i in range(len(X))]\ncategories=encode_categories(categories)\n\nlen(trimmed_data), categories.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find data too LONG OR SHORT, to exclude\nlengths = [len(series) for series in trimmed_data]\n\nindexes_too_short=np.where(np.array(lengths)<=72)[0]\nprint(indexes_too_short)\n\nindexes = indexes_too_short\nprint(len(indexes))\n\n# Create a subset of time series longer than or equal to the threshold\n#subset_data = [ts for ts in trimmed_data if len(ts) >= 26]\n\nX_noout = np.delete(X, indexes, axis=0)\nvalid_periods_noout = np.delete(valid_periods, indexes, axis=0)\ncategories_noout = np.delete(categories, indexes, axis=0)\nstart_times_noout=valid_periods_noout[:,0]\nend_times_noout=valid_periods_noout[:,1]\n\ntrimmed_data_noout = [row for i, row in enumerate(trimmed_data) if i not in indexes]\n\nprint(X_noout.shape, len(categories_noout),len(start_times_noout),len(end_times_noout) )\nprint(len(trimmed_data_noout))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the Time2Vec Layer\nclass Time2Vec(tf.keras.layers.Layer):\n    def __init__(self, kernel_size=1):\n        super(Time2Vec, self).__init__(trainable=True, name='Time2VecLayer')\n        self.k = kernel_size\n\n    def build(self, input_shape):\n        # Determine feature dimension from the last dimension of the input shape\n        feature_dim = input_shape[-1]\n        if feature_dim is None:\n            raise ValueError(\"The feature dimension of the input must be defined.\")\n\n        # Initialize weights\n        self.wb = self.add_weight(name='wb', shape=(feature_dim,), initializer='uniform', trainable=True)\n        self.bb = self.add_weight(name='bb', shape=(feature_dim,), initializer='uniform', trainable=True)\n        # Adjust the shape of wa and ba to match the kernel size\n        self.wa = self.add_weight(name='wa', shape=(feature_dim, self.k), initializer='uniform', trainable=True)\n        self.ba = self.add_weight(name='ba', shape=(self.k,), initializer='uniform', trainable=True)\n\n    def call(self, x):\n        linear = self.wb * x + self.bb\n\n        # Broadcasting to match the dimensions\n        # The shapes of wa and x are made compatible for batch matrix multiplication\n        sin_trans = tf.math.sin(tf.linalg.matmul(x, self.wa) + self.ba)\n\n        # Concatenate linear and sin_trans along the last dimension\n        return tf.concat([linear, sin_trans], axis=-1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionEmbedding(tfkl.Layer):\n    def __init__(self, maxlen, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.pos_emb = tfkl.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        return x + positions\n\nclass TransformerEncoderBlock(tfkl.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.att = tfkl.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = tfk.Sequential([tfkl.Dense(ff_dim, activation=\"relu\"), tfkl.Dense(embed_dim)])\n        self.layernorm1 = tfkl.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tfkl.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = tfkl.Dropout(rate)\n        self.dropout2 = tfkl.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\ndef create_time_series_encoder(sequence_length, embed_dim, num_heads, latent_dim):\n    encoder_inputs = tfkl.Input(shape=(sequence_length, 1))\n    x = PositionEmbedding(sequence_length, embed_dim)(encoder_inputs)\n    encoder_outputs = TransformerEncoderBlock(embed_dim, num_heads, latent_dim)(x)\n    encoder = tfk.Model(encoder_inputs, encoder_outputs, name=\"TimeSeriesEncoder\")\n\n    return encoder","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerDecoderBlock(tfkl.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.att1 = tfkl.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.att2 = tfkl.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = tfk.Sequential([tfkl.Dense(ff_dim, activation=\"relu\"), tfkl.Dense(embed_dim)])\n        self.layernorm1, self.layernorm2, self.layernorm3 = [tfkl.LayerNormalization(epsilon=1e-6) for _ in range(3)]\n        self.dropout1, self.dropout2, self.dropout3 = [tfkl.Dropout(rate) for _ in range(3)]\n\n    def call(self, inputs, encoder_outputs, training, mask=None):\n        causal_mask = self.get_causal_attention_mask(inputs)\n        padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\") if mask is not None else None\n        padding_mask = tf.minimum(padding_mask, causal_mask) if mask is not None else None\n\n        attn_output_1 = self.att1(inputs, inputs, inputs, attention_mask=causal_mask)\n        attn_output_1 = self.dropout1(attn_output_1, training=training)\n        out_1 = self.layernorm1(inputs + attn_output_1)\n\n        attn_output_2 = self.att2(out_1, encoder_outputs, encoder_outputs, attention_mask=padding_mask)\n        attn_output_2 = self.dropout2(attn_output_2, training=training)\n        out_2 = self.layernorm2(out_1 + attn_output_2)\n\n        ffn_output = self.ffn(out_2)\n        ffn_output = self.dropout3(ffn_output, training=training)\n\n        return self.layernorm3(out_2 + ffn_output)\n\n    def get_causal_attention_mask(self, inputs):\n        batch_size, sequence_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n        i, j = tf.range(sequence_length)[:, tf.newaxis], tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        return tf.tile(tf.reshape(mask, (1, sequence_length, sequence_length)), [batch_size, 1, 1])\n\ndef create_time_series_decoder(sequence_length, prediction_length, embed_dim, num_heads, latent_dim):\n    decoder_inputs = tfkl.Input(shape=(sequence_length, 1))\n    encoder_outputs = tfkl.Input(shape=(sequence_length, embed_dim))\n\n    x = PositionEmbedding(sequence_length, embed_dim)(decoder_inputs)\n    x = TransformerDecoderBlock(embed_dim, num_heads, latent_dim)(x, encoder_outputs)\n    x = tfkl.Dropout(0.5)(x)\n\n    x = tfkl.TimeDistributed(tfkl.Dense(prediction_length))(x)\n    x = tfkl.Reshape((sequence_length * prediction_length,))(x)\n    x = tfkl.Dense(prediction_length)(x)\n\n    decoder = tfk.Model([decoder_inputs, encoder_outputs], x, name=\"TimeSeriesDecoder\")\n\n    return decoder","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_transformer_time_series_model(encoder, decoder, sequence_length):\n    encoder_inputs = tfkl.Input(shape=(sequence_length, 1), name=\"encoder_inputs\")\n    decoder_inputs = tfkl.Input(shape=(sequence_length, 1), name=\"decoder_inputs\")\n\n    encoder_outputs = encoder(encoder_inputs)\n    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n\n    transformer_model = tfk.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"TimeSeriesTransformer\")\n\n    return transformer_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_length = 200\nembed_dim = 128\nnum_heads = 4\nlatent_dim = 1024\n\nprediction_length = 18","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_series_encoder = create_time_series_encoder(sequence_length, embed_dim, num_heads, latent_dim)\ntime_series_decoder = create_time_series_decoder(sequence_length, prediction_length, embed_dim, num_heads, latent_dim)\ntime_series_transformer = create_transformer_time_series_model(time_series_encoder, time_series_decoder, sequence_length)\n\ntime_series_encoder.summary()\ntime_series_decoder.summary()\ntime_series_transformer.summary()\n\ntf.keras.utils.plot_model(time_series_encoder, show_shapes=True, expand_nested=True, to_file='time_series_encoder.png')\ntf.keras.utils.plot_model(time_series_decoder, show_shapes=True, expand_nested=True, to_file='time_series_decoder.png')\ntf.keras.utils.plot_model(time_series_transformer, show_shapes=True, expand_nested=True, to_file='time_series_transformer.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_sequences(array, window, stride, start_times, end_times, categories, shuffle=True, seed=42, telescope=18):\n    df = []\n    y = []\n    cat=[]\n    \n    category_flags = encode_categories(categories)\n\n    for idx in range(len(array)):\n        start_time = start_times[idx]\n        end_time = end_times[idx]\n        category_flag = category_flags[idx]  # Extract the integer-encoded category flag\n\n        # Extract the actual non-zero part of the time series within the specified time range\n        actual_data = array[idx, start_time:end_time]\n        \n        padding_check = len(actual_data)%window\n\n        if(padding_check != 0):\n            padding_len = window - len(actual_data)%window\n            padding = np.zeros(padding_len,dtype='float32')\n            actual_data = np.concatenate((padding,actual_data))\n            assert len(actual_data) % window == 0\n\n        for i in range(0, len(actual_data) - window - telescope, stride):\n            \n            sequence = actual_data[i:i + window]\n            \n            df.append(sequence)     \n            \n            y.append(actual_data[i+window: i+ window+telescope])     \n            cat.append(categories[idx])          \n            \n    df = np.array(df)\n    y = np.array(y)\n    cat=np.array(cat)\n    cat=encode_categories(cat)\n        \n    return df, y, cat","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"myX = X_noout\nmycategories = categories_noout\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=seed)\n\ntrain_indexes=[]\nval_indexes=[]\n\nfor train_index, val_index in sss.split(myX, mycategories):\n    \n    train_indexes.append(train_index)\n    val_indexes.append(val_index)\n\n    X_train_noseq, X_val_noseq = myX[train_index], myX[val_index]\n    cat_train_noseq, cat_val_noseq = mycategories[train_index], mycategories[val_index]    \n    start_times_train_noseq, start_times_val_noseq = start_times_noout[train_index], start_times_noout[val_index]\n    end_times_train_noseq, end_times_val_noseq = end_times_noout[train_index], end_times_noout[val_index]\n\n#print(\"X_train_noseq shape:\", X_train_noseq.shape)\n#print(\"X_val_noseq shape:\", X_val_noseq.shape)\n#print(\"cat_train_noseq shape:\", cat_train_noseq.shape)\n#print(\"cat_val_noseq shape:\", cat_val_noseq.shape)\n#print(\"start_times_train_noseq shape:\", start_times_train_noseq.shape)\n#print(\"end_times_train_noseq shape:\", end_times_train_noseq.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window_size=200\nstride_size=12","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train, cats = build_sequences(X_train_noseq,                                    \n                                   window=window_size, \n                                   stride=stride_size,\n                                   start_times=start_times_train_noseq, \n                                   end_times=end_times_train_noseq, \n                                   categories=cat_train_noseq)\n\n\nX_val, y_val, cats_val = build_sequences(X_val_noseq,                                    \n                                   window=window_size, \n                                   stride=stride_size,\n                                   start_times=start_times_val_noseq, \n                                   end_times=end_times_val_noseq, \n                                   categories=cat_val_noseq)\n\n#print(X_train.shape, y_train.shape, cats.shape)\n#print( X_val.shape, y_val.shape, cats_val.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\nloss_fn = tf.keras.losses.MeanSquaredError()\nearly_stopping_callback = EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True)\nreduce_on_plateau = ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_series_transformer.compile(optimizer=optimizer, loss=loss_fn, metrics=[tf.keras.metrics.MeanSquaredError(), tf.keras.metrics.MeanAbsoluteError()])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = time_series_transformer.fit([X_train, X_train], y_train, \n                                      epochs=200, batch_size=32, \n                                      validation_data=([X_val, X_val], y_val), \n                                      callbacks=[early_stopping_callback, reduce_on_plateau])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\ntime_series_transformer.save('SubmissionModel')\nshutil.make_archive('SubmissionModel', 'zip', 'SubmissionModel')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'SubmissionModel.zip')","metadata":{},"execution_count":null,"outputs":[]}]}